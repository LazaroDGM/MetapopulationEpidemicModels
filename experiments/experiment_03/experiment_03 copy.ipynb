{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "from numba import njit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from time import time\n",
    "from statistics import mean, stdev\n",
    "import simanneal\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def fun_sir_lagrange_numba(t,y,Out, In, Beta, Gamma):\n",
    "    \n",
    "    K = Out.shape[0]\n",
    "    Out_i_k = Out.sum(axis=1)\n",
    "\n",
    "    y = y.reshape((4,K,K))\n",
    "    I_k_i = y[1].sum(axis=0)\n",
    "    N_k_i = y[3].sum(axis=0)\n",
    "    new_y = np.zeros((4,K,K))\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            if i == j:\n",
    "                new_y[0,i,i] = - Beta[i] * y[0,i,i] * I_k_i[i] / N_k_i[i] - \\\n",
    "                                y[0,i,i] * Out_i_k[i] + (In[i,] * y[0,i]).sum()\n",
    "                new_y[1,i,i] =   Beta[i] * y[0,i,i] * I_k_i[i] / N_k_i[i] - Gamma[i] * y[1,i,i] - \\\n",
    "                                y[1,i,i] * Out_i_k[i] + (In[i,] * y[1,i]).sum()\n",
    "                new_y[2,i,i] =   Gamma[i] * y[1,i,i] - \\\n",
    "                                y[2,i,i] * Out_i_k[i] + (In[i,] * y[2,i]).sum()\n",
    "                new_y[3,i,i] = - y[3,i,i] * Out_i_k[i] + (In[i,] * y[3,i]).sum()\n",
    "            else:\n",
    "                new_y[0,i,j] = - Beta[j] * y[0,i,j] * I_k_i[j] / N_k_i[j] - \\\n",
    "                                In[i,j] * y[0,i,j] + Out[i,j] * y[0,i,i]\n",
    "                new_y[1,i,j] =   Beta[j] * y[0,i,j] * I_k_i[j] / N_k_i[j] - Gamma[j] * y[1,i,j] - \\\n",
    "                                In[i,j] * y[1,i,j] + Out[i,j] * y[1,i,i]\n",
    "                new_y[2,i,j] =   Gamma[j] * y[1,i,j] - \\\n",
    "                                In[i,j] * y[2,i,j] + Out[i,j] * y[2,i,i]\n",
    "                new_y[3,i,j] = - In[i,j] * y[3,i,j] + Out[i,j] * y[3,i,i]\n",
    "\n",
    "    new_y = new_y.reshape((4*K*K,))\n",
    "    return new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(602,)\n"
     ]
    }
   ],
   "source": [
    "Beta = np.array([0.25, 0.25])\n",
    "Gamma = np.array([0.052,0.052])\n",
    "Out = np.array([\n",
    "    [0, 0.5],\n",
    "    [0.5,0]\n",
    "])\n",
    "In = Out.copy()\n",
    "y0 = np.zeros((4,2,2))\n",
    "np.fill_diagonal(y0[0], 999)\n",
    "np.fill_diagonal(y0[1], 1)\n",
    "np.fill_diagonal(y0[3], 1000)\n",
    "y0 = y0.flatten()\n",
    "ts = np.linspace(0,300,301)\n",
    "\n",
    "real_sol = solve_ivp(fun_sir_lagrange_numba,(0,300), y0, t_eval=ts, args=(Out, In, Beta, Gamma))\n",
    "real_y = real_sol.y.reshape((4, 2,2, real_sol.y.shape[1]))\n",
    "real_I = real_y[1].sum(axis=1)\n",
    "real_I = real_I.flatten()\n",
    "\n",
    "print(real_I.shape)\n",
    "\n",
    "def fitness(value):\n",
    "    new_Beta = np.array([value[0], value[0]])\n",
    "    new_Gamma = np.array([value[1], value[1]])\n",
    "    #new_Out = np.array([\n",
    "    #    [0, value[4]],\n",
    "    #    [value[5],0]\n",
    "    #])\n",
    "    #new_In = np.array([\n",
    "    #    [0, value[6]],\n",
    "    #    [value[7],0]\n",
    "    #])\n",
    "    estimate_sol = solve_ivp(fun_sir_lagrange_numba,(0,300), y0, t_eval=ts, args=(Out, In, new_Beta, new_Gamma))\n",
    "    estimate_y = estimate_sol.y.reshape((4, 2,2, estimate_sol.y.shape[1]))\n",
    "    estimate_I = estimate_y[1].sum(axis=1)\n",
    "    estimate_I = estimate_I.flatten()\n",
    "    #er =  mean_squared_error(real_I, estimate_I)\n",
    "    return mean_squared_error(real_I, estimate_I)\n",
    "    return real_I - estimate_I\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Temperature        Energy    Accept   Improve     Elapsed   Remaining\n",
      "    66.00000        216.90     0.05%     0.00%     0:01:56     0:22:15"
     ]
    }
   ],
   "source": [
    "import simanneal\n",
    "import random\n",
    "\n",
    "class MySA(simanneal.Annealer):\n",
    "    def move(self):\n",
    "        x, y = self.state\n",
    "        x_new = x + (0.5 - random.random())  # Randomly perturb x\n",
    "        x_new = max(x_new, 1e-4)\n",
    "\n",
    "        y_new = y + (0.5 - random.random())  # Randomly perturb y\n",
    "        y_new = max(y_new, 1e-4)\n",
    "        \n",
    "        self.state = (x_new, y_new)\n",
    "\n",
    "    def energy(self):\n",
    "        return fitness(self.state)\n",
    "        \n",
    "\n",
    "# Initial state\n",
    "initial_state = (1.0, 1.0)\n",
    "\n",
    "# Create an instance of the annealer\n",
    "sa = MySA(initial_state)\n",
    "\n",
    "# Set the parameters for the annealing process\n",
    "sa.set_schedule(sa.auto(minutes=1))\n",
    "\n",
    "# Perform the annealing\n",
    "state, e = sa.anneal()\n",
    "\n",
    "print(\"Optimal state:\", state)\n",
    "print(\"Minimum energy:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 86, initial cost 5.8138e+06, final cost 2.3460e-25, first-order optimality 1.18e-08.\n",
      "[0.25  0.25  0.052 0.052]\n",
      "2.3459722803168234e-25\n",
      "1.1771622233312537e-08\n"
     ]
    }
   ],
   "source": [
    "r = least_squares(fitness, x0= np.array([random.random(),random.random(),random.random(),random.random()]), bounds=(0,1), verbose=1)\n",
    "print(r.x)\n",
    "print(r.cost)\n",
    "print(r.optimality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No constraints given.\n",
      "New best for swarm at iteration 1: [0.48104744 0.46083275 0.09718038 0.68005328] 15085.98607394335\n",
      "New best for swarm at iteration 1: [0.32042845 0.54566321 0.23218832 0.18541053] 13734.030558488703\n",
      "New best for swarm at iteration 1: [0.65393888 0.18172047 0.20879289 0.09072557] 13373.1790129557\n",
      "Best after iteration 1: [0.65393888 0.18172047 0.20879289 0.09072557] 13373.1790129557\n",
      "New best for swarm at iteration 2: [0.68550063 0.01826015 0.24613134 0.        ] 9528.108726247106\n",
      "Best after iteration 2: [0.68550063 0.01826015 0.24613134 0.        ] 9528.108726247106\n",
      "New best for swarm at iteration 3: [0.35594249 0.28809518 0.13308701 0.07515605] 5704.224403467264\n",
      "New best for swarm at iteration 3: [0.21676803 0.26382832 0.08027295 0.        ] 1504.8934301981224\n",
      "Best after iteration 3: [0.21676803 0.26382832 0.08027295 0.        ] 1504.8934301981224\n",
      "New best for swarm at iteration 4: [0.22022838 0.24815808 0.09836775 0.        ] 395.6625295584415\n",
      "Best after iteration 4: [0.22022838 0.24815808 0.09836775 0.        ] 395.6625295584415\n",
      "Best after iteration 5: [0.22022838 0.24815808 0.09836775 0.        ] 395.6625295584415\n",
      "New best for swarm at iteration 6: [0.22180767 0.24282638 0.10683743 0.        ] 374.96160389422135\n",
      "Best after iteration 6: [0.22180767 0.24282638 0.10683743 0.        ] 374.96160389422135\n",
      "New best for swarm at iteration 7: [0.23814492 0.24335353 0.10954873 0.        ] 109.2199199691475\n",
      "Best after iteration 7: [0.23814492 0.24335353 0.10954873 0.        ] 109.2199199691475\n",
      "New best for swarm at iteration 8: [0.21774767 0.26042968 0.11123896 0.        ] 101.9861338330274\n",
      "Best after iteration 8: [0.21774767 0.26042968 0.11123896 0.        ] 101.9861338330274\n",
      "New best for swarm at iteration 9: [0.24000368 0.25011795 0.10498851 0.        ] 46.81890753913567\n",
      "New best for swarm at iteration 9: [0.25375849 0.24114819 0.10474546 0.        ] 35.42516437402809\n",
      "Best after iteration 9: [0.25375849 0.24114819 0.10474546 0.        ] 35.42516437402809\n",
      "New best for swarm at iteration 10: [0.24781818 0.24610551 0.10623991 0.        ] 23.675569629230928\n",
      "Best after iteration 10: [0.24781818 0.24610551 0.10623991 0.        ] 23.675569629230928\n",
      "New best for swarm at iteration 11: [0.25231781 0.24632063 0.10594947 0.        ] 17.13503205792613\n",
      "New best for swarm at iteration 11: [0.26940995 0.22919557 0.10829919 0.        ] 7.681064088426408\n",
      "New best for swarm at iteration 11: [0.26496391 0.23524598 0.10788353 0.        ] 4.175522199527712\n",
      "Best after iteration 11: [0.26496391 0.23524598 0.10788353 0.        ] 4.175522199527712\n",
      "New best for swarm at iteration 12: [0.25236671 0.24717965 0.1104484  0.        ] 0.8366960515648263\n",
      "Best after iteration 12: [0.25236671 0.24717965 0.1104484  0.        ] 0.8366960515648263\n",
      "Best after iteration 13: [0.25236671 0.24717965 0.1104484  0.        ] 0.8366960515648263\n",
      "New best for swarm at iteration 14: [0.25265466 0.24890892 0.10584637 0.00370936] 0.47704736103596335\n",
      "New best for swarm at iteration 14: [0.25572046 0.24473437 0.10979697 0.        ] 0.18386650155462456\n",
      "Best after iteration 14: [0.25572046 0.24473437 0.10979697 0.        ] 0.18386650155462456\n",
      "Best after iteration 15: [0.25572046 0.24473437 0.10979697 0.        ] 0.18386650155462456\n",
      "New best for swarm at iteration 16: [0.25370107 0.24648271 0.10989687 0.        ] 0.15609803535904346\n",
      "Best after iteration 16: [0.25370107 0.24648271 0.10989687 0.        ] 0.15609803535904346\n",
      "New best for swarm at iteration 17: [0.25429511 0.24608987 0.10999675 0.        ] 0.13767914222622124\n",
      "Best after iteration 17: [0.25429511 0.24608987 0.10999675 0.        ] 0.13767914222622124\n",
      "New best for swarm at iteration 18: [2.54435428e-01 2.46196836e-01 1.09798664e-01 3.88646556e-05] 0.03786817396297991\n",
      "Best after iteration 18: [2.54435428e-01 2.46196836e-01 1.09798664e-01 3.88646556e-05] 0.03786817396297991\n",
      "Best after iteration 19: [2.54435428e-01 2.46196836e-01 1.09798664e-01 3.88646556e-05] 0.03786817396297991\n",
      "New best for swarm at iteration 20: [2.54244237e-01 2.46466578e-01 1.09869645e-01 1.96473235e-05] 0.01743699437367689\n",
      "Best after iteration 20: [2.54244237e-01 2.46466578e-01 1.09869645e-01 1.96473235e-05] 0.01743699437367689\n",
      "New best for swarm at iteration 21: [2.54670293e-01 2.46329882e-01 1.09872620e-01 8.08366961e-06] 0.002792588890050286\n",
      "New best for swarm at iteration 21: [2.54459945e-01 2.46519806e-01 1.09868152e-01 2.37582604e-05] 0.0027846734931121515\n",
      "Best after iteration 21: [2.54459945e-01 2.46519806e-01 1.09868152e-01 2.37582604e-05] 0.0027846734931121515\n",
      "Best after iteration 22: [2.54459945e-01 2.46519806e-01 1.09868152e-01 2.37582604e-05] 0.0027846734931121515\n",
      "Best after iteration 23: [2.54459945e-01 2.46519806e-01 1.09868152e-01 2.37582604e-05] 0.0027846734931121515\n",
      "Best after iteration 24: [2.54459945e-01 2.46519806e-01 1.09868152e-01 2.37582604e-05] 0.0027846734931121515\n",
      "New best for swarm at iteration 25: [2.54436758e-01 2.46517300e-01 1.09867893e-01 2.40161619e-05] 0.002416662709913328\n",
      "Best after iteration 25: [2.54436758e-01 2.46517300e-01 1.09867893e-01 2.40161619e-05] 0.002416662709913328\n",
      "New best for swarm at iteration 26: [2.54009406e-01 2.46868410e-01 1.09882959e-01 2.57558488e-05] 0.002094415744899776\n",
      "Best after iteration 26: [2.54009406e-01 2.46868410e-01 1.09882959e-01 2.57558488e-05] 0.002094415744899776\n",
      "Best after iteration 27: [2.54009406e-01 2.46868410e-01 1.09882959e-01 2.57558488e-05] 0.002094415744899776\n",
      "Best after iteration 28: [2.54009406e-01 2.46868410e-01 1.09882959e-01 2.57558488e-05] 0.002094415744899776\n",
      "Best after iteration 29: [2.54009406e-01 2.46868410e-01 1.09882959e-01 2.57558488e-05] 0.002094415744899776\n",
      "New best for swarm at iteration 30: [2.53984085e-01 2.46884593e-01 1.09879943e-01 2.49037665e-05] 0.0020250446165572663\n",
      "Best after iteration 30: [2.53984085e-01 2.46884593e-01 1.09879943e-01 2.49037665e-05] 0.0020250446165572663\n",
      "New best for swarm at iteration 31: [2.53930340e-01 2.46925812e-01 1.09883494e-01 2.48513764e-05] 0.002009186946657988\n",
      "Best after iteration 31: [2.53930340e-01 2.46925812e-01 1.09883494e-01 2.48513764e-05] 0.002009186946657988\n",
      "New best for swarm at iteration 32: [2.53928212e-01 2.46934231e-01 1.09883696e-01 2.47445479e-05] 0.0020000051981704948\n",
      "Best after iteration 32: [2.53928212e-01 2.46934231e-01 1.09883696e-01 2.47445479e-05] 0.0020000051981704948\n",
      "New best for swarm at iteration 33: [2.53941666e-01 2.46924318e-01 1.09877142e-01 2.50538489e-05] 0.0019981030944529092\n",
      "Best after iteration 33: [2.53941666e-01 2.46924318e-01 1.09877142e-01 2.50538489e-05] 0.0019981030944529092\n",
      "New best for swarm at iteration 34: [2.53940675e-01 2.46921470e-01 1.09879776e-01 2.45261790e-05] 0.001986614420025241\n",
      "New best for swarm at iteration 34: [2.53785586e-01 2.47038844e-01 1.09873111e-01 2.47940357e-05] 0.0019842138693983683\n",
      "Best after iteration 34: [2.53785586e-01 2.47038844e-01 1.09873111e-01 2.47940357e-05] 0.0019842138693983683\n",
      "Best after iteration 35: [2.53785586e-01 2.47038844e-01 1.09873111e-01 2.47940357e-05] 0.0019842138693983683\n",
      "New best for swarm at iteration 36: [2.53880074e-01 2.46962460e-01 1.09878133e-01 2.47373824e-05] 0.001967481929205565\n",
      "New best for swarm at iteration 36: [2.53842827e-01 2.47003790e-01 1.09877242e-01 2.52618422e-05] 0.0019290718989493656\n",
      "Best after iteration 36: [2.53842827e-01 2.47003790e-01 1.09877242e-01 2.52618422e-05] 0.0019290718989493656\n",
      "Best after iteration 37: [2.53842827e-01 2.47003790e-01 1.09877242e-01 2.52618422e-05] 0.0019290718989493656\n",
      "Best after iteration 38: [2.53842827e-01 2.47003790e-01 1.09877242e-01 2.52618422e-05] 0.0019290718989493656\n",
      "New best for swarm at iteration 39: [2.53812997e-01 2.47021682e-01 1.09876451e-01 2.51876376e-05] 0.0019213124761769604\n",
      "Best after iteration 39: [2.53812997e-01 2.47021682e-01 1.09876451e-01 2.51876376e-05] 0.0019213124761769604\n",
      "New best for swarm at iteration 40: [2.53818423e-01 2.47022026e-01 1.09876893e-01 2.51801427e-05] 0.0019169175221501998\n",
      "Best after iteration 40: [2.53818423e-01 2.47022026e-01 1.09876893e-01 2.51801427e-05] 0.0019169175221501998\n",
      "New best for swarm at iteration 41: [2.53812068e-01 2.47023273e-01 1.09876759e-01 2.52144777e-05] 0.0019160524174732516\n",
      "New best for swarm at iteration 41: [2.53815940e-01 2.47024822e-01 1.09877963e-01 2.52319534e-05] 0.0019071864984613104\n",
      "New best for swarm at iteration 41: [2.53785328e-01 2.47043814e-01 1.09877845e-01 2.53342091e-05] 0.0018971673357841452\n",
      "Best after iteration 41: [2.53785328e-01 2.47043814e-01 1.09877845e-01 2.53342091e-05] 0.0018971673357841452\n",
      "New best for swarm at iteration 42: [2.53775941e-01 2.47053943e-01 1.09878283e-01 2.52448250e-05] 0.001884206303162785\n",
      "Best after iteration 42: [2.53775941e-01 2.47053943e-01 1.09878283e-01 2.52448250e-05] 0.001884206303162785\n",
      "New best for swarm at iteration 43: [2.53771247e-01 2.47059007e-01 1.09878502e-01 2.52001329e-05] 0.0018792452468258244\n",
      "Best after iteration 43: [2.53771247e-01 2.47059007e-01 1.09878502e-01 2.52001329e-05] 0.0018792452468258244\n",
      "New best for swarm at iteration 44: [2.53768900e-01 2.47061539e-01 1.09878611e-01 2.51777868e-05] 0.0018771446594261522\n",
      "Best after iteration 44: [2.53768900e-01 2.47061539e-01 1.09878611e-01 2.51777868e-05] 0.0018771446594261522\n",
      "New best for swarm at iteration 45: [2.53781042e-01 2.47052301e-01 1.09879512e-01 2.52425868e-05] 0.0018769123260314454\n",
      "New best for swarm at iteration 45: [2.53769521e-01 2.47061820e-01 1.09878971e-01 2.51758037e-05] 0.0018747047604312215\n",
      "Best after iteration 45: [2.53769521e-01 2.47061820e-01 1.09878971e-01 2.51758037e-05] 0.0018747047604312215\n",
      "New best for swarm at iteration 46: [2.53761308e-01 2.47069226e-01 1.09878954e-01 2.52225947e-05] 0.0018705037687892967\n",
      "Best after iteration 46: [2.53761308e-01 2.47069226e-01 1.09878954e-01 2.52225947e-05] 0.0018705037687892967\n",
      "New best for swarm at iteration 47: [2.53758898e-01 2.47070675e-01 1.09879149e-01 2.52259906e-05] 0.0018674552703891883\n",
      "Best after iteration 47: [2.53758898e-01 2.47070675e-01 1.09879149e-01 2.52259906e-05] 0.0018674552703891883\n",
      "New best for swarm at iteration 48: [2.53757693e-01 2.47071399e-01 1.09879247e-01 2.52276885e-05] 0.0018660522141249049\n",
      "New best for swarm at iteration 48: [2.53753109e-01 2.47074028e-01 1.09879457e-01 2.52192442e-05] 0.0018627664510063795\n",
      "Best after iteration 48: [2.53753109e-01 2.47074028e-01 1.09879457e-01 2.52192442e-05] 0.0018627664510063795\n",
      "New best for swarm at iteration 49: [2.53753316e-01 2.47075261e-01 1.09879447e-01 2.52184268e-05] 0.0018624513471102002\n",
      "Best after iteration 49: [2.53753316e-01 2.47075261e-01 1.09879447e-01 2.52184268e-05] 0.0018624513471102002\n",
      "New best for swarm at iteration 50: [2.53750955e-01 2.47074410e-01 1.09879724e-01 2.52328673e-05] 0.001861882872720876\n",
      "New best for swarm at iteration 50: [2.53751858e-01 2.47076556e-01 1.09879669e-01 2.52220935e-05] 0.0018601086990975135\n",
      "Best after iteration 50: [2.53751858e-01 2.47076556e-01 1.09879669e-01 2.52220935e-05] 0.0018601086990975135\n",
      "New best for swarm at iteration 51: [2.53750277e-01 2.47076020e-01 1.09879879e-01 2.52287011e-05] 0.0018588742664360385\n",
      "Best after iteration 51: [2.53750277e-01 2.47076020e-01 1.09879879e-01 2.52287011e-05] 0.0018588742664360385\n",
      "New best for swarm at iteration 52: [2.53748706e-01 2.47078718e-01 1.09879871e-01 2.52373509e-05] 0.001856968276616447\n",
      "Best after iteration 52: [2.53748706e-01 2.47078718e-01 1.09879871e-01 2.52373509e-05] 0.001856968276616447\n",
      "New best for swarm at iteration 53: [2.53747748e-01 2.47078519e-01 1.09880043e-01 2.52361752e-05] 0.0018560094356921983\n",
      "New best for swarm at iteration 53: [2.53746140e-01 2.47079537e-01 1.09879959e-01 2.52373146e-05] 0.00185596636668276\n",
      "New best for swarm at iteration 53: [2.53745078e-01 2.47081435e-01 1.09879997e-01 2.52357614e-05] 0.0018543254032257176\n",
      "Best after iteration 53: [2.53745078e-01 2.47081435e-01 1.09879997e-01 2.52357614e-05] 0.0018543254032257176\n",
      "New best for swarm at iteration 54: [2.53744238e-01 2.47081647e-01 1.09880102e-01 2.52368569e-05] 0.0018535265040405592\n",
      "New best for swarm at iteration 54: [2.53742118e-01 2.47083030e-01 1.09880109e-01 2.52380849e-05] 0.001852687952947538\n",
      "Best after iteration 54: [2.53742118e-01 2.47083030e-01 1.09880109e-01 2.52380849e-05] 0.001852687952947538\n",
      "New best for swarm at iteration 55: [2.53740637e-01 2.47083827e-01 1.09880165e-01 2.52392467e-05] 0.0018520972584427924\n",
      "Best after iteration 55: [2.53740637e-01 2.47083827e-01 1.09880165e-01 2.52392467e-05] 0.0018520972584427924\n",
      "New best for swarm at iteration 56: [2.53735883e-01 2.47086763e-01 1.09880279e-01 2.52303714e-05] 0.001850776376244343\n",
      "Best after iteration 56: [2.53735883e-01 2.47086763e-01 1.09880279e-01 2.52303714e-05] 0.001850776376244343\n",
      "New best for swarm at iteration 57: [2.53725834e-01 2.47096359e-01 1.09880309e-01 2.52327656e-05] 0.0018429367198683935\n",
      "Best after iteration 57: [2.53725834e-01 2.47096359e-01 1.09880309e-01 2.52327656e-05] 0.0018429367198683935\n",
      "New best for swarm at iteration 58: [2.53720810e-01 2.47101158e-01 1.09880324e-01 2.52339627e-05] 0.0018395662579116104\n",
      "Best after iteration 58: [2.53720810e-01 2.47101158e-01 1.09880324e-01 2.52339627e-05] 0.0018395662579116104\n",
      "New best for swarm at iteration 59: [2.53718298e-01 2.47103557e-01 1.09880332e-01 2.52345612e-05] 0.0018380184095585321\n",
      "New best for swarm at iteration 59: [2.53722751e-01 2.47103800e-01 1.09878619e-01 2.76038624e-05] 0.0018362017574741418\n",
      "New best for swarm at iteration 59: [2.53720495e-01 2.47104723e-01 1.09880096e-01 2.64075496e-05] 0.0018332481685339732\n",
      "Best after iteration 59: [2.53720495e-01 2.47104723e-01 1.09880096e-01 2.64075496e-05] 0.0018332481685339732\n",
      "New best for swarm at iteration 60: [2.53718868e-01 2.47107457e-01 1.09880774e-01 2.63524578e-05] 0.0018321136185754673\n",
      "New best for swarm at iteration 60: [2.53716910e-01 2.47108392e-01 1.09880108e-01 2.69921710e-05] 0.0018289105156533419\n",
      "Best after iteration 60: [2.53716910e-01 2.47108392e-01 1.09880108e-01 2.69921710e-05] 0.0018289105156533419\n",
      "New best for swarm at iteration 61: [2.53716508e-01 2.47106817e-01 1.09880163e-01 2.66285953e-05] 0.001828097119489714\n",
      "New best for swarm at iteration 61: [2.53714950e-01 2.47109957e-01 1.09880138e-01 2.71302605e-05] 0.001826836952220014\n",
      "Best after iteration 61: [2.53714950e-01 2.47109957e-01 1.09880138e-01 2.71302605e-05] 0.001826836952220014\n",
      "New best for swarm at iteration 62: [2.53715674e-01 2.47108740e-01 1.09880125e-01 2.72696701e-05] 0.0018258101850423284\n",
      "New best for swarm at iteration 62: [2.53713998e-01 2.47110218e-01 1.09880153e-01 2.72396482e-05] 0.0018249130148186663\n",
      "New best for swarm at iteration 62: [2.53711761e-01 2.47112655e-01 1.09880161e-01 2.74759560e-05] 0.0018234562721546436\n",
      "Best after iteration 62: [2.53711761e-01 2.47112655e-01 1.09880161e-01 2.74759560e-05] 0.0018234562721546436\n",
      "New best for swarm at iteration 63: [2.53712620e-01 2.47110242e-01 1.09880103e-01 2.72232304e-05] 0.001823273887625322\n",
      "New best for swarm at iteration 63: [2.53713142e-01 2.47110200e-01 1.09880441e-01 2.72590607e-05] 0.0018227627691943767\n",
      "New best for swarm at iteration 63: [2.53708835e-01 2.47110357e-01 1.09880254e-01 2.74382737e-05] 0.001822075815980555\n",
      "New best for swarm at iteration 63: [2.53709961e-01 2.47113086e-01 1.09880196e-01 2.73232518e-05] 0.001821399127489965\n",
      "New best for swarm at iteration 63: [2.53710320e-01 2.47113449e-01 1.09880155e-01 2.77163625e-05] 0.0018211709186197723\n",
      "New best for swarm at iteration 63: [2.53709527e-01 2.47114078e-01 1.09880189e-01 2.75587650e-05] 0.0018210274320801577\n",
      "New best for swarm at iteration 63: [2.53709342e-01 2.47112263e-01 1.09880157e-01 2.74432827e-05] 0.0018201664460607352\n",
      "Best after iteration 63: [2.53709342e-01 2.47112263e-01 1.09880157e-01 2.74432827e-05] 0.0018201664460607352\n",
      "New best for swarm at iteration 64: [2.53708446e-01 2.47111770e-01 1.09880291e-01 2.76737937e-05] 0.001819606426887689\n",
      "New best for swarm at iteration 64: [2.53707692e-01 2.47112861e-01 1.09880235e-01 2.75285169e-05] 0.0018190450454464659\n",
      "New best for swarm at iteration 64: [2.53707337e-01 2.47115208e-01 1.09880203e-01 2.76824005e-05] 0.0018184883037910848\n",
      "New best for swarm at iteration 64: [2.53707701e-01 2.47113378e-01 1.09880163e-01 2.80207420e-05] 0.001817902046377322\n",
      "Best after iteration 64: [2.53707701e-01 2.47113378e-01 1.09880163e-01 2.80207420e-05] 0.001817902046377322\n",
      "New best for swarm at iteration 65: [2.53706577e-01 2.47113796e-01 1.09880200e-01 2.77939722e-05] 0.0018177994696696096\n",
      "New best for swarm at iteration 65: [2.53704216e-01 2.47115026e-01 1.09880178e-01 2.76584631e-05] 0.0018173439125348987\n",
      "New best for swarm at iteration 65: [2.53706362e-01 2.47115855e-01 1.09880153e-01 2.81830358e-05] 0.0018168008785941455\n",
      "Best after iteration 65: [2.53706362e-01 2.47115855e-01 1.09880153e-01 2.81830358e-05] 0.0018168008785941455\n",
      "New best for swarm at iteration 66: [2.53703683e-01 2.47116481e-01 1.09880188e-01 2.79739861e-05] 0.0018153826439364664\n",
      "Best after iteration 66: [2.53703683e-01 2.47116481e-01 1.09880188e-01 2.79739861e-05] 0.0018153826439364664\n",
      "New best for swarm at iteration 67: [2.53704148e-01 2.47116271e-01 1.09880168e-01 2.81771648e-05] 0.0018153527980857929\n",
      "New best for swarm at iteration 67: [2.53703576e-01 2.47117162e-01 1.09880186e-01 2.81893106e-05] 0.0018147620435274132\n",
      "Best after iteration 67: [2.53703576e-01 2.47117162e-01 1.09880186e-01 2.81893106e-05] 0.0018147620435274132\n",
      "New best for swarm at iteration 68: [2.53703752e-01 2.47117348e-01 1.09880192e-01 2.83359011e-05] 0.0018146986787709624\n",
      "New best for swarm at iteration 68: [2.53703274e-01 2.47117618e-01 1.09880187e-01 2.82965153e-05] 0.001814415248965953\n",
      "New best for swarm at iteration 68: [2.53702275e-01 2.47117826e-01 1.09880194e-01 2.81550826e-05] 0.0018140867983970678\n",
      "New best for swarm at iteration 68: [2.53702158e-01 2.47118585e-01 1.09880191e-01 2.84502500e-05] 0.0018135521210372295\n",
      "Best after iteration 68: [2.53702158e-01 2.47118585e-01 1.09880191e-01 2.84502500e-05] 0.0018135521210372295\n",
      "New best for swarm at iteration 69: [2.53701258e-01 2.47118955e-01 1.09880192e-01 2.84561237e-05] 0.0018130413432578623\n",
      "Best after iteration 69: [2.53701258e-01 2.47118955e-01 1.09880192e-01 2.84561237e-05] 0.0018130413432578623\n",
      "New best for swarm at iteration 70: [2.53701173e-01 2.47119517e-01 1.09880165e-01 2.84281939e-05] 0.0018128796390379919\n",
      "New best for swarm at iteration 70: [2.53700786e-01 2.47119204e-01 1.09880186e-01 2.84471966e-05] 0.0018127889007812136\n",
      "Best after iteration 70: [2.53700786e-01 2.47119204e-01 1.09880186e-01 2.84471966e-05] 0.0018127889007812136\n",
      "New best for swarm at iteration 71: [2.53700443e-01 2.47119663e-01 1.09880173e-01 2.84562993e-05] 0.0018124673665022907\n",
      "New best for swarm at iteration 71: [2.53699740e-01 2.47120649e-01 1.09880174e-01 2.81455484e-05] 0.0018122125406564028\n",
      "New best for swarm at iteration 71: [2.53699452e-01 2.47120064e-01 1.09880193e-01 2.83249170e-05] 0.0018120844441766914\n",
      "Best after iteration 71: [2.53699452e-01 2.47120064e-01 1.09880193e-01 2.83249170e-05] 0.0018120844441766914\n",
      "New best for swarm at iteration 72: [2.53698990e-01 2.47120667e-01 1.09880174e-01 2.83214396e-05] 0.0018116483443906394\n",
      "Best after iteration 72: [2.53698990e-01 2.47120667e-01 1.09880174e-01 2.83214396e-05] 0.0018116483443906394\n",
      "New best for swarm at iteration 73: [2.53698614e-01 2.47120676e-01 1.09880174e-01 2.84093852e-05] 0.0018115283324900318\n",
      "New best for swarm at iteration 73: [2.53698814e-01 2.47120741e-01 1.09880176e-01 2.83789708e-05] 0.0018115242584226692\n",
      "Stopping search: Swarm best objective change less than 1e-08\n",
      "Optimal x: [2.53698814e-01 2.47120741e-01 1.09880176e-01 2.83789708e-05]\n",
      "Optimal y: 0.0018115242584226692\n",
      "Minimum value: 0.0018115242584226692\n"
     ]
    }
   ],
   "source": [
    "import pyswarm\n",
    "\n",
    "# Define la función de costo que queremos minimizar\n",
    "def objective_function(params):\n",
    "    x, y = params\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Llama a la función de optimización de enjambre de partículas\n",
    "x_opt, y_opt = pyswarm.pso(fitness, lb=[0.0, 0.0,0.0, 0.0], ub=[1, 1,1, 1],swarmsize=40, maxiter=100, debug=True)\n",
    "\n",
    "print(\"Optimal x:\", x_opt)\n",
    "print(\"Optimal y:\", y_opt)\n",
    "print(\"Minimum value:\", fitness(x_opt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[189], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mprint\u001b[39m(io(\u001b[39m1234\u001b[39;49m, \u001b[39m566\u001b[39;49m))\n\u001b[1;32m     25\u001b[0m \u001b[39m# Realizar la optimización con Evolución Diferencial\u001b[39;00m\n\u001b[1;32m     26\u001b[0m result \u001b[39m=\u001b[39m differential_evolution(fitness, [(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m), (\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)],\n\u001b[1;32m     27\u001b[0m                                 maxiter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,atol\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,disp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,polish\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m                                 workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, callback\u001b[39m=\u001b[39mio)\n",
      "Cell \u001b[0;32mIn[189], line 18\u001b[0m, in \u001b[0;36mio\u001b[0;34m(xk, convergence)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mio\u001b[39m(xk, convergence):\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mif\u001b[39;00m fitness(xk) \u001b[39m<\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[1;32m     19\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDATANTA\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[162], line 23\u001b[0m, in \u001b[0;36mfitness\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfitness\u001b[39m(value):\n\u001b[0;32m---> 23\u001b[0m     new_Beta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([value[\u001b[39m0\u001b[39;49m], value[\u001b[39m0\u001b[39m]])\n\u001b[1;32m     24\u001b[0m     new_Gamma \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([value[\u001b[39m1\u001b[39m], value[\u001b[39m1\u001b[39m]])\n\u001b[1;32m     25\u001b[0m     \u001b[39m#new_Out = np.array([\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39m#    [0, value[4]],\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m#    [value[5],0]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[39m#    [value[7],0]\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[39m#])\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import differential_evolution\n",
    "#import sko.PSO\n",
    "\n",
    "\n",
    "\n",
    "# Define la función de costo que queremos minimizar\n",
    "def objective_function(params):\n",
    "    x, y = params\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Definir los límites de búsqueda para x e y\n",
    "bounds = [(-10, 10), (-10, 10)]\n",
    "\n",
    "my = np.zeros((2,))\n",
    "\n",
    "def fitness_DE(value):\n",
    "    new_Beta = np.array([value[0], value[0]])\n",
    "    new_Gamma = np.array([value[1], value[1]])\n",
    "    estimate_sol = solve_ivp(fun_sir_lagrange_numba,(0,300), y0, t_eval=ts, args=(Out, In, new_Beta, new_Gamma))\n",
    "    estimate_y = estimate_sol.y.reshape((4, 2*2, estimate_sol.y.shape[1]))\n",
    "    estimate_I = estimate_y[1].sum(axis=0)\n",
    "    return mean_squared_error(real_I, estimate_I)\n",
    "\n",
    "def io(xk, convergence):\n",
    "    if fitness_DE(xk) < 10:\n",
    "        print('DATANTA')\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "print(io(1234, 566))\n",
    "\n",
    "# Realizar la optimización con Evolución Diferencial\n",
    "result = differential_evolution(fitness_DE, [(0, 1), (0, 1)],\n",
    "                                maxiter=10,atol=0.01,disp=True,polish=True,\n",
    "                                workers=1, callback=io)\n",
    "print(result.items())\n",
    "print(\"Optimal parameters:\", result.x)\n",
    "print(\"Minimum value:\", result.fun)\n",
    "my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function differential_evolution in module scipy.optimize._differentialevolution:\n",
      "\n",
      "differential_evolution(func, bounds, args=(), strategy='best1bin', maxiter=1000, popsize=15, tol=0.01, mutation=(0.5, 1), recombination=0.7, seed=None, callback=None, disp=False, polish=True, init='latinhypercube', atol=0, updating='immediate', workers=1, constraints=(), x0=None, *, integrality=None, vectorized=False)\n",
      "    Finds the global minimum of a multivariate function.\n",
      "    \n",
      "    The differential evolution method [1]_ is stochastic in nature. It does\n",
      "    not use gradient methods to find the minimum, and can search large areas\n",
      "    of candidate space, but often requires larger numbers of function\n",
      "    evaluations than conventional gradient-based techniques.\n",
      "    \n",
      "    The algorithm is due to Storn and Price [2]_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    func : callable\n",
      "        The objective function to be minimized. Must be in the form\n",
      "        ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "        and ``args`` is a tuple of any additional fixed parameters needed to\n",
      "        completely specify the function. The number of parameters, N, is equal\n",
      "        to ``len(x)``.\n",
      "    bounds : sequence or `Bounds`\n",
      "        Bounds for variables. There are two ways to specify the bounds:\n",
      "        1. Instance of `Bounds` class.\n",
      "        2. ``(min, max)`` pairs for each element in ``x``, defining the finite\n",
      "        lower and upper bounds for the optimizing argument of `func`.\n",
      "        The total number of bounds is used to determine the number of\n",
      "        parameters, N.\n",
      "    args : tuple, optional\n",
      "        Any additional fixed parameters needed to\n",
      "        completely specify the objective function.\n",
      "    strategy : str, optional\n",
      "        The differential evolution strategy to use. Should be one of:\n",
      "    \n",
      "            - 'best1bin'\n",
      "            - 'best1exp'\n",
      "            - 'rand1exp'\n",
      "            - 'randtobest1exp'\n",
      "            - 'currenttobest1exp'\n",
      "            - 'best2exp'\n",
      "            - 'rand2exp'\n",
      "            - 'randtobest1bin'\n",
      "            - 'currenttobest1bin'\n",
      "            - 'best2bin'\n",
      "            - 'rand2bin'\n",
      "            - 'rand1bin'\n",
      "    \n",
      "        The default is 'best1bin'.\n",
      "    maxiter : int, optional\n",
      "        The maximum number of generations over which the entire population is\n",
      "        evolved. The maximum number of function evaluations (with no polishing)\n",
      "        is: ``(maxiter + 1) * popsize * N``\n",
      "    popsize : int, optional\n",
      "        A multiplier for setting the total population size. The population has\n",
      "        ``popsize * N`` individuals. This keyword is overridden if an\n",
      "        initial population is supplied via the `init` keyword. When using\n",
      "        ``init='sobol'`` the population size is calculated as the next power\n",
      "        of 2 after ``popsize * N``.\n",
      "    tol : float, optional\n",
      "        Relative tolerance for convergence, the solving stops when\n",
      "        ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "        where and `atol` and `tol` are the absolute and relative tolerance\n",
      "        respectively.\n",
      "    mutation : float or tuple(float, float), optional\n",
      "        The mutation constant. In the literature this is also known as\n",
      "        differential weight, being denoted by F.\n",
      "        If specified as a float it should be in the range [0, 2].\n",
      "        If specified as a tuple ``(min, max)`` dithering is employed. Dithering\n",
      "        randomly changes the mutation constant on a generation by generation\n",
      "        basis. The mutation constant for that generation is taken from\n",
      "        ``U[min, max)``. Dithering can help speed convergence significantly.\n",
      "        Increasing the mutation constant increases the search radius, but will\n",
      "        slow down convergence.\n",
      "    recombination : float, optional\n",
      "        The recombination constant, should be in the range [0, 1]. In the\n",
      "        literature this is also known as the crossover probability, being\n",
      "        denoted by CR. Increasing this value allows a larger number of mutants\n",
      "        to progress into the next generation, but at the risk of population\n",
      "        stability.\n",
      "    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional\n",
      "        If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "        singleton is used.\n",
      "        If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "        seeded with `seed`.\n",
      "        If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "        that instance is used.\n",
      "        Specify `seed` for repeatable minimizations.\n",
      "    disp : bool, optional\n",
      "        Prints the evaluated `func` at every iteration.\n",
      "    callback : callable, `callback(xk, convergence=val)`, optional\n",
      "        A function to follow the progress of the minimization. ``xk`` is\n",
      "        the best solution found so far. ``val`` represents the fractional\n",
      "        value of the population convergence.  When ``val`` is greater than one\n",
      "        the function halts. If callback returns `True`, then the minimization\n",
      "        is halted (any polishing is still carried out).\n",
      "    polish : bool, optional\n",
      "        If True (default), then `scipy.optimize.minimize` with the `L-BFGS-B`\n",
      "        method is used to polish the best population member at the end, which\n",
      "        can improve the minimization slightly. If a constrained problem is\n",
      "        being studied then the `trust-constr` method is used instead. For large\n",
      "        problems with many constraints, polishing can take a long time due to\n",
      "        the Jacobian computations.\n",
      "    init : str or array-like, optional\n",
      "        Specify which type of population initialization is performed. Should be\n",
      "        one of:\n",
      "    \n",
      "            - 'latinhypercube'\n",
      "            - 'sobol'\n",
      "            - 'halton'\n",
      "            - 'random'\n",
      "            - array specifying the initial population. The array should have\n",
      "              shape ``(S, N)``, where S is the total population size and N is\n",
      "              the number of parameters.\n",
      "              `init` is clipped to `bounds` before use.\n",
      "    \n",
      "        The default is 'latinhypercube'. Latin Hypercube sampling tries to\n",
      "        maximize coverage of the available parameter space.\n",
      "    \n",
      "        'sobol' and 'halton' are superior alternatives and maximize even more\n",
      "        the parameter space. 'sobol' will enforce an initial population\n",
      "        size which is calculated as the next power of 2 after\n",
      "        ``popsize * N``. 'halton' has no requirements but is a bit less\n",
      "        efficient. See `scipy.stats.qmc` for more details.\n",
      "    \n",
      "        'random' initializes the population randomly - this has the drawback\n",
      "        that clustering can occur, preventing the whole of parameter space\n",
      "        being covered. Use of an array to specify a population could be used,\n",
      "        for example, to create a tight bunch of initial guesses in an location\n",
      "        where the solution is known to exist, thereby reducing time for\n",
      "        convergence.\n",
      "    atol : float, optional\n",
      "        Absolute tolerance for convergence, the solving stops when\n",
      "        ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "        where and `atol` and `tol` are the absolute and relative tolerance\n",
      "        respectively.\n",
      "    updating : {'immediate', 'deferred'}, optional\n",
      "        If ``'immediate'``, the best solution vector is continuously updated\n",
      "        within a single generation [4]_. This can lead to faster convergence as\n",
      "        trial vectors can take advantage of continuous improvements in the best\n",
      "        solution.\n",
      "        With ``'deferred'``, the best solution vector is updated once per\n",
      "        generation. Only ``'deferred'`` is compatible with parallelization or\n",
      "        vectorization, and the `workers` and `vectorized` keywords can\n",
      "        over-ride this option.\n",
      "    \n",
      "        .. versionadded:: 1.2.0\n",
      "    \n",
      "    workers : int or map-like callable, optional\n",
      "        If `workers` is an int the population is subdivided into `workers`\n",
      "        sections and evaluated in parallel\n",
      "        (uses `multiprocessing.Pool <multiprocessing>`).\n",
      "        Supply -1 to use all available CPU cores.\n",
      "        Alternatively supply a map-like callable, such as\n",
      "        `multiprocessing.Pool.map` for evaluating the population in parallel.\n",
      "        This evaluation is carried out as ``workers(func, iterable)``.\n",
      "        This option will override the `updating` keyword to\n",
      "        ``updating='deferred'`` if ``workers != 1``.\n",
      "        This option overrides the `vectorized` keyword if ``workers != 1``.\n",
      "        Requires that `func` be pickleable.\n",
      "    \n",
      "        .. versionadded:: 1.2.0\n",
      "    \n",
      "    constraints : {NonLinearConstraint, LinearConstraint, Bounds}\n",
      "        Constraints on the solver, over and above those applied by the `bounds`\n",
      "        kwd. Uses the approach by Lampinen [5]_.\n",
      "    \n",
      "        .. versionadded:: 1.4.0\n",
      "    \n",
      "    x0 : None or array-like, optional\n",
      "        Provides an initial guess to the minimization. Once the population has\n",
      "        been initialized this vector replaces the first (best) member. This\n",
      "        replacement is done even if `init` is given an initial population.\n",
      "        ``x0.shape == (N,)``.\n",
      "    \n",
      "        .. versionadded:: 1.7.0\n",
      "    \n",
      "    integrality : 1-D array, optional\n",
      "        For each decision variable, a boolean value indicating whether the\n",
      "        decision variable is constrained to integer values. The array is\n",
      "        broadcast to ``(N,)``.\n",
      "        If any decision variables are constrained to be integral, they will not\n",
      "        be changed during polishing.\n",
      "        Only integer values lying between the lower and upper bounds are used.\n",
      "        If there are no integer values lying between the bounds then a\n",
      "        `ValueError` is raised.\n",
      "    \n",
      "        .. versionadded:: 1.9.0\n",
      "    \n",
      "    vectorized : bool, optional\n",
      "        If ``vectorized is True``, `func` is sent an `x` array with\n",
      "        ``x.shape == (N, S)``, and is expected to return an array of shape\n",
      "        ``(S,)``, where `S` is the number of solution vectors to be calculated.\n",
      "        If constraints are applied, each of the functions used to construct\n",
      "        a `Constraint` object should accept an `x` array with\n",
      "        ``x.shape == (N, S)``, and return an array of shape ``(M, S)``, where\n",
      "        `M` is the number of constraint components.\n",
      "        This option is an alternative to the parallelization offered by\n",
      "        `workers`, and may help in optimization speed by reducing interpreter\n",
      "        overhead from multiple function calls. This keyword is ignored if\n",
      "        ``workers != 1``.\n",
      "        This option will override the `updating` keyword to\n",
      "        ``updating='deferred'``.\n",
      "        See the notes section for further discussion on when to use\n",
      "        ``'vectorized'``, and when to use ``'workers'``.\n",
      "    \n",
      "        .. versionadded:: 1.9.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a `OptimizeResult` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes. If `polish`\n",
      "        was employed, and a lower minimum was obtained by the polishing, then\n",
      "        OptimizeResult also contains the ``jac`` attribute.\n",
      "        If the eventual solution does not satisfy the applied constraints\n",
      "        ``success`` will be `False`.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Differential evolution is a stochastic population based method that is\n",
      "    useful for global optimization problems. At each pass through the\n",
      "    population the algorithm mutates each candidate solution by mixing with\n",
      "    other candidate solutions to create a trial candidate. There are several\n",
      "    strategies [3]_ for creating trial candidates, which suit some problems\n",
      "    more than others. The 'best1bin' strategy is a good starting point for\n",
      "    many systems. In this strategy two members of the population are randomly\n",
      "    chosen. Their difference is used to mutate the best member (the 'best' in\n",
      "    'best1bin'), :math:`b_0`, so far:\n",
      "    \n",
      "    .. math::\n",
      "    \n",
      "        b' = b_0 + mutation * (population[rand0] - population[rand1])\n",
      "    \n",
      "    A trial vector is then constructed. Starting with a randomly chosen ith\n",
      "    parameter the trial is sequentially filled (in modulo) with parameters\n",
      "    from ``b'`` or the original candidate. The choice of whether to use ``b'``\n",
      "    or the original candidate is made with a binomial distribution (the 'bin'\n",
      "    in 'best1bin') - a random number in [0, 1) is generated. If this number is\n",
      "    less than the `recombination` constant then the parameter is loaded from\n",
      "    ``b'``, otherwise it is loaded from the original candidate. The final\n",
      "    parameter is always loaded from ``b'``. Once the trial candidate is built\n",
      "    its fitness is assessed. If the trial is better than the original candidate\n",
      "    then it takes its place. If it is also better than the best overall\n",
      "    candidate it also replaces that.\n",
      "    To improve your chances of finding a global minimum use higher `popsize`\n",
      "    values, with higher `mutation` and (dithering), but lower `recombination`\n",
      "    values. This has the effect of widening the search radius, but slowing\n",
      "    convergence.\n",
      "    By default the best solution vector is updated continuously within a single\n",
      "    iteration (``updating='immediate'``). This is a modification [4]_ of the\n",
      "    original differential evolution algorithm which can lead to faster\n",
      "    convergence as trial vectors can immediately benefit from improved\n",
      "    solutions. To use the original Storn and Price behaviour, updating the best\n",
      "    solution once per iteration, set ``updating='deferred'``.\n",
      "    The ``'deferred'`` approach is compatible with both parallelization and\n",
      "    vectorization (``'workers'`` and ``'vectorized'`` keywords). These may\n",
      "    improve minimization speed by using computer resources more efficiently.\n",
      "    The ``'workers'`` distribute calculations over multiple processors. By\n",
      "    default the Python `multiprocessing` module is used, but other approaches\n",
      "    are also possible, such as the Message Passing Interface (MPI) used on\n",
      "    clusters [6]_ [7]_. The overhead from these approaches (creating new\n",
      "    Processes, etc) may be significant, meaning that computational speed\n",
      "    doesn't necessarily scale with the number of processors used.\n",
      "    Parallelization is best suited to computationally expensive objective\n",
      "    functions. If the objective function is less expensive, then\n",
      "    ``'vectorized'`` may aid by only calling the objective function once per\n",
      "    iteration, rather than multiple times for all the population members; the\n",
      "    interpreter overhead is reduced.\n",
      "    \n",
      "    .. versionadded:: 0.15.0\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Differential evolution, Wikipedia,\n",
      "           http://en.wikipedia.org/wiki/Differential_evolution\n",
      "    .. [2] Storn, R and Price, K, Differential Evolution - a Simple and\n",
      "           Efficient Heuristic for Global Optimization over Continuous Spaces,\n",
      "           Journal of Global Optimization, 1997, 11, 341 - 359.\n",
      "    .. [3] http://www1.icsi.berkeley.edu/~storn/code.html\n",
      "    .. [4] Wormington, M., Panaccione, C., Matney, K. M., Bowen, D. K., -\n",
      "           Characterization of structures from X-ray scattering data using\n",
      "           genetic algorithms, Phil. Trans. R. Soc. Lond. A, 1999, 357,\n",
      "           2827-2848\n",
      "    .. [5] Lampinen, J., A constraint handling approach for the differential\n",
      "           evolution algorithm. Proceedings of the 2002 Congress on\n",
      "           Evolutionary Computation. CEC'02 (Cat. No. 02TH8600). Vol. 2. IEEE,\n",
      "           2002.\n",
      "    .. [6] https://mpi4py.readthedocs.io/en/stable/\n",
      "    .. [7] https://schwimmbad.readthedocs.io/en/latest/\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function is implemented in `rosen` in `scipy.optimize`.\n",
      "    \n",
      "    >>> import numpy as np\n",
      "    >>> from scipy.optimize import rosen, differential_evolution\n",
      "    >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "    >>> result = differential_evolution(rosen, bounds)\n",
      "    >>> result.x, result.fun\n",
      "    (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "    \n",
      "    Now repeat, but with parallelization.\n",
      "    \n",
      "    >>> result = differential_evolution(rosen, bounds, updating='deferred',\n",
      "    ...                                 workers=2)\n",
      "    >>> result.x, result.fun\n",
      "    (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "    \n",
      "    Let's do a constrained minimization.\n",
      "    \n",
      "    >>> from scipy.optimize import LinearConstraint, Bounds\n",
      "    \n",
      "    We add the constraint that the sum of ``x[0]`` and ``x[1]`` must be less\n",
      "    than or equal to 1.9.  This is a linear constraint, which may be written\n",
      "    ``A @ x <= 1.9``, where ``A = array([[1, 1]])``.  This can be encoded as\n",
      "    a `LinearConstraint` instance:\n",
      "    \n",
      "    >>> lc = LinearConstraint([[1, 1]], -np.inf, 1.9)\n",
      "    \n",
      "    Specify limits using a `Bounds` object.\n",
      "    \n",
      "    >>> bounds = Bounds([0., 0.], [2., 2.])\n",
      "    >>> result = differential_evolution(rosen, bounds, constraints=lc,\n",
      "    ...                                 seed=1)\n",
      "    >>> result.x, result.fun\n",
      "    (array([0.96632622, 0.93367155]), 0.0011352416852625719)\n",
      "    \n",
      "    Next find the minimum of the Ackley function\n",
      "    (https://en.wikipedia.org/wiki/Test_functions_for_optimization).\n",
      "    \n",
      "    >>> def ackley(x):\n",
      "    ...     arg1 = -0.2 * np.sqrt(0.5 * (x[0] ** 2 + x[1] ** 2))\n",
      "    ...     arg2 = 0.5 * (np.cos(2. * np.pi * x[0]) + np.cos(2. * np.pi * x[1]))\n",
      "    ...     return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n",
      "    >>> bounds = [(-5, 5), (-5, 5)]\n",
      "    >>> result = differential_evolution(ackley, bounds, seed=1)\n",
      "    >>> result.x, result.fun\n",
      "    (array([0., 0.]), 4.440892098500626e-16)\n",
      "    \n",
      "    The Ackley function is written in a vectorized manner, so the\n",
      "    ``'vectorized'`` keyword can be employed. Note the reduced number of\n",
      "    function evaluations.\n",
      "    \n",
      "    >>> result = differential_evolution(\n",
      "    ...     ackley, bounds, vectorized=True, updating='deferred', seed=1\n",
      "    ... )\n",
      "    >>> result.x, result.fun\n",
      "    (array([0., 0.]), 4.440892098500626e-16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(differential_evolution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
